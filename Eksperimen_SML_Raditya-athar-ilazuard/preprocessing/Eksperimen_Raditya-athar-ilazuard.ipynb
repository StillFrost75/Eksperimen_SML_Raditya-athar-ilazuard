{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      },
      "source": [
        "# **1. Perkenalan Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hssSDn-5n3HR"
      },
      "source": [
        "Tahap pertama, Anda harus mencari dan menggunakan dataset dengan ketentuan sebagai berikut:\n",
        "\n",
        "1. **Sumber Dataset**:  \n",
        "   #Dataset dapat diperoleh dari berbagai sumber, seperti public repositories (*Kaggle*, *UCI ML Repository*, *Open Data*) atau data primer yang Anda kumpulkan sendiri.\n",
        "   Kaggle link: https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data?resource=download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKADPWcFKlj3"
      },
      "source": [
        "# **2. Import Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgA3ERnVn84N"
      },
      "source": [
        "Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning atau deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3YIEnAFKrKL"
      },
      "source": [
        "# **3. Memuat Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey3ItwTen_7E"
      },
      "source": [
        "Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.\n",
        "\n",
        "Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.\n",
        "\n",
        "Jika dataset berupa unstructured data, silakan sesuaikan dengan format seperti kelas Machine Learning Pengembangan atau Machine Learning Terapan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "outputs": [],
      "source": [
        "dataset_path = '../heart.csv'\n",
        "\n",
        "try:\n",
        "    # Membaca dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    \n",
        "    # Menampilkan 5 baris pertama untuk pengecekan\n",
        "    print(\"Dataset berhasil dimuat!\")\n",
        "    print(f\"Ukuran dataset: {df.shape[0]} baris, {df.shape[1]} kolom\")\n",
        "    display(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File tidak ditemukan di {dataset_path}.\")\n",
        "    print(\"Pastikan path file sudah benar dan file sudah di-upload.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZkbJLpK9UR"
      },
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.\n",
        "\n",
        "Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "outputs": [],
      "source": [
        "# 1. Cek Informasi Dasar (Tipe Data & Null)\n",
        "print(\"=== Info Dataset ===\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Cek Statistik Deskriptif (Mean, Min, Max, dll)\n",
        "print(\"\\n=== Statistik Deskriptif ===\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Cek Missing Values\n",
        "print(\"\\n=== Cek Missing Values ===\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Visualisasi Target (Apakah seimbang atau imbalance?)\n",
        "print(\"\\n=== Nama Kolom ===\")\n",
        "print(df.columns)\n",
        "\n",
        "target_col = 'num' # Target [0=no heart disease; 1,2,3,4 = stages of heart disease ]\n",
        "#\n",
        "if target_col in df.columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x=target_col, data=df, palette='viridis')\n",
        "    plt.title('Distribusi Label Target (0=no heart disease; 1,2,3,4 = stages of heart disease)')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"Kolom '{target_col}' tidak ditemukan. Mohon cek nama kolom output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Visualisasi Korelasi (Heatmap)\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df.corr(numeric_only=True)\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Heatmap Korelasi Antar Fitur')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgHfgnSK3ip"
      },
      "source": [
        "# **5. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8pGV0-iDLz"
      },
      "outputs": [],
      "source": [
        "# A. CLEANING AWAL\n",
        "# ----------------\n",
        "df_clean = df.copy()\n",
        "\n",
        "# 1. Ubah Target menjadi Binary (0 = Sehat, 1 = Sakit Jantung)\n",
        "# Nilai 1, 2, 3, 4 dianggap '1' (Sakit)\n",
        "df_clean['num'] = df_clean['num'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# 2. Buang kolom yang tidak perlu atau terlalu banyak missing values (>50%)\n",
        "# 'id', 'dataset': Identitas (tidak guna untuk prediksi)\n",
        "# 'ca', 'thal', 'slope': Missing value terlalu banyak (lihat hasil EDA)\n",
        "cols_to_drop = ['id', 'dataset', 'ca', 'thal', 'slope']\n",
        "df_clean = df_clean.drop(columns=cols_to_drop)\n",
        "\n",
        "print(\"Ukuran dataset setelah cleaning awal:\", df_clean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# B. DATA SPLITTING\n",
        "# -----------------\n",
        "X = df_clean.drop(columns=['num'])\n",
        "y = df_clean['num']\n",
        "\n",
        "# Split 80% Train, 20% Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Jumlah Data Train: {X_train.shape[0]}\")\n",
        "print(f\"Jumlah Data Test: {X_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C. MEMBUAT PIPELINE PREPROCESSING\n",
        "# ---------------------------------\n",
        "numeric_features = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak']\n",
        "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang']\n",
        "\n",
        "# Pipeline untuk Data Numerik:\n",
        "# 1. Isi missing value dengan Median\n",
        "# 2. Standarisasi skala (StandardScaler)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline untuk Data Kategorikal:\n",
        "# 1. Isi missing value dengan Modus (most_frequent)\n",
        "# 2. Ubah teks jadi angka (OneHotEncoder)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Gabungkan keduanya\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# D. TERAPKAN PIPELINE\n",
        "# --------------------\n",
        "# Fit hanya pada training data untuk mencegah data leakage\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Ambil nama fitur baru setelah OneHotEncoding (untuk keperluan display saja)\n",
        "# Ini agak tricky, tapi berguna untuk melihat hasil\n",
        "ohe_feature_names = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
        "all_feature_names = numeric_features + list(ohe_feature_names)\n",
        "\n",
        "# Konversi kembali ke DataFrame agar enak dilihat\n",
        "X_train_df = pd.DataFrame(X_train_processed, columns=all_feature_names)\n",
        "X_test_df = pd.DataFrame(X_test_processed, columns=all_feature_names)\n",
        "\n",
        "print(\"\\n=== Contoh Data Siap Latih (X_train) ===\")\n",
        "display(X_train_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E. SIMPAN DATASET HASIL PREPROCESSING\n",
        "y_train_reset = y_train.reset_index(drop=True)\n",
        "y_test_reset = y_test.reset_index(drop=True)\n",
        "\n",
        "# Masukkan kembali target ke dataframe\n",
        "X_train_df['num'] = y_train_reset\n",
        "X_test_df['num'] = y_test_reset\n",
        "\n",
        "# Simpan ke file CSV\n",
        "X_train_df.to_csv('train_processed.csv', index=False)\n",
        "X_test_df.to_csv('test_processed.csv', index=False)\n",
        "\n",
        "print(\"\\nDataset hasil preprocessing berhasil disimpan!\")\n",
        "print(\"File: 'train_processed.csv' dan 'test_processed.csv' telah dibuat.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
